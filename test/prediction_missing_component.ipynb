{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf95d59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1eff37",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # Tutorial notebook for predicting a missing component in a regularly varying random vector -- work in progress\n",
    "\n",
    "\n",
    "# \n",
    "# import os\n",
    "# os.getcwd()\n",
    "# os.chdir(\"../\")\n",
    "# os.getcwd()\n",
    "# \n",
    "\n",
    "# \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import MLExtreme as mlx\n",
    "# \n",
    "#\n",
    "# Norm function: L2 norm. change `norm_order` to 1 or inf for variants.\n",
    "# NB: inf is only applicable to the linear transform below, not the nonlinear\n",
    "# transform\n",
    "#\n",
    "norm_order = 2\n",
    "\n",
    "\n",
    "def norm_func(x):\n",
    "    return np.linalg.norm(x, ord=norm_order, axis=1)\n",
    "\n",
    "\n",
    "# \n",
    "# Data generation\n",
    "np.random.seed(42)\n",
    "n = int(4 * 10**4)\n",
    "Dim = 2\n",
    "k = 2\n",
    "alpha = 2\n",
    "# Mu0 =  np.array([[0.1,  0.7], [0.6,  0.4], [0.9, 0.1] ])\n",
    "Mu0 = np.array([[0.1, 0.7, 0.2], [0.2, 0.1, 0.7]])\n",
    "wei0 = np.ones(2)\n",
    "Mu, wei = mlx.normalize_param_dirimix(Mu0, wei0)\n",
    "lnu = np.log(10 / np.min(Mu, axis=1))\n",
    "# define  adversarial bulk angular measure, see function gen_rv_dirimix\n",
    "Mu_bulk = np.array([[0.7, 0.1, 0.2], [0.1, 0.2, 0.7]])\n",
    "\n",
    "XZ = mlx.gen_rv_dirimix(alpha, Mu, wei, lnu, scale_weight_noise=1,\n",
    "                        index_weight_noise=3, Mu_bulk=Mu_bulk,\n",
    "                        size=n)\n",
    "X = XZ[:, :-1]\n",
    "Z = XZ[:, -1]\n",
    "\n",
    "\n",
    "# ## plot limit angular measure and bulk angular measure\n",
    "mlx.plot_pdf_dirimix_3D(Mu, wei, lnu, n_points=100)\n",
    "mlx.plot_pdf_dirimix_3D(Mu_bulk, wei, lnu, n_points=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4243cb1a",
   "metadata": {},
   "source": [
    "## Tranforming the target :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd55e267",
   "metadata": {},
   "source": [
    "### Option 1: learn a prediction model for a linear tranform  of the target,\n",
    "### y =  Target / ||X||.\n",
    "### appropriate when  Target / ||X|| is bounded or at least not clearly\n",
    "### heavy tailed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0917bb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "y1 = mlx.transform_target_lin(Z, X, norm_func)\n",
    "\n",
    "# test\n",
    "Z1 = mlx.inv_transform_target_lin(y1, X, norm_func)\n",
    "np.sum((Z1 - Z)**2)  # OK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ef4276",
   "metadata": {},
   "source": [
    "### Option 2:  Nonlinear transformation :\n",
    "### y = z / (||(x,z) ||) where z is the original target,\n",
    "### and x the covariate\n",
    "Appropriate when z/||x||  may not be considered bounded.\n",
    "if ||.|| is the L_q norm, the inverse transform is:\n",
    " z =  y/ (1 - y**q)**(1/q)  * || x ||\n",
    "NB: the infinite norm is not appropriate here, only L_q norms, q in [1, inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed49470",
   "metadata": {},
   "outputs": [],
   "source": [
    "y2 = mlx.transform_target_nonlin(Z, X, 2)\n",
    "\n",
    "# test\n",
    "Z2 = mlx.inv_transform_target_nonlin(y2, X, 2)\n",
    "np.sum((Z2 - Z)**2) # OK\n",
    "\n",
    "# boundedness assumption for y1?\n",
    "plt.hist(y1)\n",
    "plt.show()\n",
    "# warning here: Some of the (comparatively) largest values of y1 may cause\n",
    "# instability.\n",
    "\n",
    "# what about y2?\n",
    "plt.hist(y2)\n",
    "plt.show()\n",
    "# as expected y2 it is strictly comprised in [0,1]\n",
    "\n",
    "# Vizualisation of  y1 / y2 versus x. \n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "scatter1 = axes[0].scatter(X[:, 0], X[:, 1], c=y1, cmap='gray', alpha=0.5)\n",
    "axes[0].set_title('Scatter Plot with y1')\n",
    "axes[0].set_xlabel('X1')\n",
    "axes[0].set_ylabel('X2')\n",
    "scatter2 = axes[1].scatter(X[:, 0], X[:, 1], c=y2, cmap='gray', alpha=0.5)\n",
    "axes[1].set_title('Scatter Plot with y2')\n",
    "axes[1].set_xlabel('X1')\n",
    "axes[1].set_ylabel('X2')\n",
    "fig.colorbar(scatter1, ax=axes[0])\n",
    "fig.colorbar(scatter2, ax=axes[1])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# similar pattern "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24aedfb3",
   "metadata": {},
   "source": [
    "### Train/test split: here (simulated data) half/half"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccdfede5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for y1: Splitting the data into training and test sets\n",
    "split = 0.5\n",
    "ntrain = n * (1-split)\n",
    "ntest = n * split \n",
    "X_train, X_test, y_train1, y_test1 = train_test_split(X, y1,\n",
    "                                                      test_size=split,\n",
    "                                                      random_state=1)\n",
    "# for y2: Splitting the data into training and test sets (same splits)\n",
    "_ , _ , y_train2, y_test2 = train_test_split(X, y2,\n",
    "                                             test_size=split,\n",
    "                                             random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c6a613",
   "metadata": {},
   "source": [
    "## Choice of k, Episode 1.\n",
    "##############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38889cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (for adaptive choice of k_train, see below)\n",
    "# We suggest the following rule of thumb:\n",
    "# Select k_train as the largest k such that a spearman test  of independence\n",
    "# between the k largest radii and the target  cannot reject the null.\n",
    "# At the prediction test, theoretical guarantees cover any k_pred < k_train. \n",
    "\n",
    "ratios = np.linspace(50/ntrain, 0.2, num=50)\n",
    "test_indep = mlx.test_indep_target_radius(X_train, y_train1, ratios,  norm_func)\n",
    "# plot \n",
    "mlx.plot_indep_target_radius(test_indep)\n",
    "# k ~ 1500 maximum on this example.\n",
    "\n",
    "pvals = test_indep['pvalues']\n",
    "i_max = np.max(np.where(pvals>0.05)[0])\n",
    "k_max = (ratios[i_max]*ntrain).astype(int)\n",
    "ratio_max = ratios[i_max]\n",
    "print(k_max, mlx.round_signif(ratio_max, 2))\n",
    "# indeed. \n",
    "\n",
    "# NB: one may check that results are identical with y2: the spearman test only\n",
    "# relies on ranks and y2 is a non-decreasing transform of y1. \n",
    "\n",
    "\n",
    "# Set training and prediction ratios accordingly: \n",
    "ratio_train = 0.08\n",
    "ratio_test = 0.02   # higher quantile than training quantile \n",
    "norm_X = norm_func(X)\n",
    "thresh_predict = np.quantile(norm_X, 1-ratio_test)\n",
    "k_train = ratio_train * ntrain\n",
    "\n",
    "# Pick  a classifier model in sklearn, previously imported\n",
    "model = GradientBoostingRegressor()\n",
    "# Regressor class initialization\n",
    "regressor1 = mlx.Regressor(model, norm_func) \n",
    "regressor2 = mlx.Regressor(model, norm_func)\n",
    " \n",
    "# Training the models\n",
    "thresh_train, _, X_train_extreme = regressor1.fit(X_train, y_train1, k=k_train)\n",
    "regressor2.fit(X_train, y_train2, k=k_train)\n",
    "\n",
    "# Prediction on the test data\n",
    "y_pred_extreme1,  X_test_extreme, mask_test = regressor1.predict(\n",
    "                                                X_test, thresh_predict)\n",
    "y_pred_extreme2, _, _  = regressor2.predict(X_test, thresh_predict)\n",
    "\n",
    "# Evaluation of Mean Squared Error (MSE)\n",
    "y_test_extreme1 = y_test1[mask_test]  # Filter test labels based on mask\n",
    "y_test_extreme2 = y_test2[mask_test]  # Filter test labels based on mask\n",
    "MSE1 = mean_squared_error(y_test_extreme1, y_pred_extreme1)\n",
    "MSE2 = mean_squared_error(y_test_extreme2, y_pred_extreme2)\n",
    "print(f'MSE1: {MSE1:.4f}')\n",
    "print(f'MSE2: {MSE2:.4f}')\n",
    "\n",
    "# Display regression results\n",
    "regressor1.plot_predictions(y_test_extreme1, y_pred_extreme1)\n",
    "regressor2.plot_predictions(y_test_extreme2, y_pred_extreme2)\n",
    "# prediction of y2 is more efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f99de24",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# ## How about performance in terms of the original target?\n",
    "# back to original scale:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33898aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_test_extreme = mlx.inv_transform_target_lin(y_test_extreme1, X_test_extreme,\n",
    "                                              norm_func)\n",
    "z_pred_extreme1 = mlx.inv_transform_target_lin(y_pred_extreme1, X_test_extreme,\n",
    "                                               norm_func)\n",
    "z_pred_extreme2 = mlx.inv_transform_target_nonlin(y_pred_extreme2,\n",
    "                                                  X_test_extreme,\n",
    "                                                  norm_order)\n",
    "# Compare with naive method: train model on the full dataset,\n",
    "# predict on extreme covariates.\n",
    "naive = model\n",
    "naive.fit(X_train, mlx.inv_transform_target_lin(y_train1, X_train, norm_func))\n",
    "z_pred_extreme_naive = naive.predict(X_test_extreme)\n",
    "MSE_naive = mean_squared_error(z_test_extreme, z_pred_extreme_naive)\n",
    "print(f'MSE for naive sklearn model trained on full data: {MSE_naive:.4f}')\n",
    "MSE_meth1 = mean_squared_error(z_test_extreme, z_pred_extreme1)\n",
    "print(f'MSE original target with linear method 1: {MSE_meth1:.4f}')\n",
    "MSE_meth2 = mean_squared_error(z_test_extreme, z_pred_extreme2)\n",
    "print(f'MSE original target  with nonlinear method 2: {MSE_meth2:.4f}')\n",
    "regressor1.plot_predictions(z_test_extreme, z_pred_extreme_naive)\n",
    "regressor1.plot_predictions(z_test_extreme, z_pred_extreme1)\n",
    "regressor2.plot_predictions(z_test_extreme, z_pred_extreme2)\n",
    "\n",
    "# Clearly method 2 beats method 1 and naive method,  \n",
    "# However naive and  method 1 are comparable .\n",
    "\n",
    "why?\n",
    "ang_pred = X_test_extreme[:, 0] / (norm_func(X_test_extreme))\n",
    "plt.scatter(ang_pred, z_test_extreme, c='black', alpha=0.5, label='true z')\n",
    "plt.scatter(ang_pred, z_pred_extreme1, c='blue', alpha=0.5, label = 'pred1')\n",
    "plt.scatter(ang_pred, z_pred_extreme2, c='red', alpha=0.5, label='pred2')\n",
    "plt.xlabel('angle X[0]/ || X||')\n",
    "plt.ylabel(' target (original z) values')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "# bad predictions with method 1 for largest z values; \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9729e4",
   "metadata": {},
   "source": [
    "## CHOICE OF K - Episode 2\n",
    "# cross-validation choice of k, thresh_predict fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63bf93ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (re) setting the prediction threshold. \n",
    "ratio_test = 0.02\n",
    "norm_X = norm_func(X)\n",
    "thresh_predict = np.quantile(norm_X, 1-ratio_test)\n",
    "\n",
    "# set the range of candidates training threshold, using X_train only. \n",
    "ratio_train = np.geomspace(0.02, 0.25, num=10)\n",
    "k_train = (ntrain * ratio_train).astype(int)\n",
    "regressor1 = mlx.Regressor(model, norm_func)\n",
    "regressor2 = mlx.Regressor(model, norm_func)\n",
    "\n",
    "# cross-validation on the training set. \n",
    "kscores1 = []\n",
    "kscores_sd1 = []\n",
    "kscores2 = []\n",
    "kscores_sd2 = []\n",
    "\n",
    "!time consuming\n",
    "for k in k_train:\n",
    "    mean_scores, sd_mean_scores, _ = regressor1.cross_validate(\n",
    "        X_train, y_train1, k=k, thresh_predict=thresh_predict,\n",
    "        scoring=mean_squared_error,\n",
    "        random_state=k*7)\n",
    "    kscores_sd1.append(sd_mean_scores)\n",
    "    kscores1.append(mean_scores)\n",
    "    mean_scores, sd_mean_scores, _ = regressor2.cross_validate(\n",
    "        X_train, y_train2, k=k, thresh_predict=thresh_predict,\n",
    "        scoring=mean_squared_error,\n",
    "        random_state=k*7)\n",
    "    kscores_sd2.append(sd_mean_scores)\n",
    "    kscores2.append(mean_scores)\n",
    "\n",
    "kscores1 = np.array(kscores1)\n",
    "kscores_sd1 = np.array(kscores_sd1)\n",
    "kscores2 = np.array(kscores2)\n",
    "kscores_sd2 = np.array(kscores_sd2)\n",
    "\n",
    "plt.plot(k_train, kscores1)\n",
    "plt.fill_between(k_train, kscores1 + 1.64 * kscores_sd1,\n",
    "                 kscores1 - 1.64 * kscores_sd1, color='blue', alpha=0.2)\n",
    "plt.title(\"optimal k, linear method 1\") \n",
    "plt.show()\n",
    "\n",
    "plt.plot(k_train, kscores2)\n",
    "plt.fill_between(k_train, kscores2 + 1.64 * kscores_sd2,\n",
    "                 kscores2 - 1.64 * kscores_sd2, color='blue', alpha=0.2)\n",
    "plt.title(\"optimal k, nonlinear method 2\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "i_opt1 = np.argmin(kscores1)\n",
    "k_opt1 = k_train[i_opt1]\n",
    "print(f'optimal k linear transfom method 1: {k_opt1}')\n",
    "\n",
    "i_opt2 = np.argmin(kscores2)\n",
    "k_opt2 = k_train[i_opt2]\n",
    "print(f'optimal k linear transfom method 2: {k_opt2}')\n",
    "\n",
    "\n",
    "# method 1: retrain (on training set ) and evaluate (on test set)\n",
    "# using  optimal k\n",
    "regressor1 = mlx.Regressor(model, norm_func) \n",
    "regressor2 = mlx.Regressor(model, norm_func)\n",
    "thresh_train1, _ , X_train_extreme1 = regressor1.fit(X_train, y_train1,\n",
    "                                                     k=k_opt1)\n",
    "y_pred_extreme1,  X_test_extreme,  mask_test = regressor1.predict(\n",
    "                                            X_test, thresh_predict)\n",
    "y_test_extreme1 = y_test1[mask_test]  # Filter test labels based on mask\n",
    "MSE1 = mean_squared_error(y_test_extreme1, y_pred_extreme1)\n",
    "print(f'MSE transformed scale linear method1 : {MSE1:.4f}')\n",
    "\n",
    "# method  2: idem \n",
    "thresh_train2, _ , X_train_extreme2 = regressor2.fit(X_train, y_train2,\n",
    "                                                     k=k_opt2)\n",
    "y_pred_extreme2,  X_test_extreme,  mask_test = regressor2.predict(\n",
    "                                            X_test, thresh_predict)\n",
    "y_test_extreme2 = y_test2[mask_test]  # Filter test labels based on mask\n",
    "MSE2 = mean_squared_error(y_test_extreme2, y_pred_extreme2)\n",
    "print(f'MSE transformed scale nonlinear method2 : {MSE2:.4f}')\n",
    "\n",
    "# \n",
    "regressor1.plot_predictions(y_test_extreme1, y_pred_extreme1)\n",
    "# much better than with rule-of-thumb choice of k\n",
    "regressor2.plot_predictions(y_test_extreme2, y_pred_extreme2)\n",
    "\n",
    "# # back to original scale\n",
    "z_test_extreme = mlx.inv_transform_target_lin(y_test_extreme1, X_test_extreme,\n",
    "                                              norm_func)\n",
    "z_pred_extreme1 = mlx.inv_transform_target_lin(y_pred_extreme1, X_test_extreme,\n",
    "                                               norm_func)\n",
    "z_pred_extreme2 = mlx.inv_transform_target_nonlin(y_pred_extreme2,\n",
    "                                                  X_test_extreme,\n",
    "                                                  norm_order)\n",
    "                                           \n",
    "MSE_meth1 = mean_squared_error(z_test_extreme, z_pred_extreme1)\n",
    "print(f'MSE original target with linear method 1: {MSE_meth1:.4f}')\n",
    "MSE_meth2 = mean_squared_error(z_test_extreme, z_pred_extreme2)\n",
    "print(f'MSE original target  with nonlinear method 2: {MSE_meth2:.4f}')\n",
    "print(f'(recall) MSE for naive sklearn model trained on full data: {MSE_naive:.4f}')\n",
    "\n",
    "regressor1.plot_predictions(z_test_extreme, z_pred_extreme1)\n",
    "regressor2.plot_predictions(z_test_extreme, z_pred_extreme2)\n",
    "regressor1.plot_predictions(z_test_extreme, z_pred_extreme_naive)\n",
    "\n",
    "# now the gap between method1 and 2 is not so significative.\n",
    "# both largely outperform the naive method (by a factor about 4). \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
